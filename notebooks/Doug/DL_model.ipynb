{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs Deep Learning on nutau and nue samples created by Dasha. \n",
    "\n",
    "This code runs in a python 3.7 conda environment constructed as follows:\n",
    "- conda create --name tf-gpu tensorflow-gpu keras scikit-learn matplotlib ipykernel nb_conda_kernels [Pillow]\n",
    "- conda activate tf-gpu\n",
    "(Pillow is for image manipulation for making heat maps, but I haven't got it to work yet.  Can remove from environment.)\n",
    "\n",
    "To run with multiple GPUs on CyberLAMP you must specify the \"nodes\" and \"gpus\" qualifiers in the same chunk.  For example:\n",
    "- qsub -I -A cyberlamp -l qos=cl_higpu -l nodes=1:ppn=1:gpus=4:shared -l mem=24gb -l walltime=4:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set which GPU to use.  This probably needs to be done before any other CUDA vars get defined.\n",
    "# Use the command \"nvidia-smi\" to get association of a particular GPU with a particular number.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2272015596816976\n"
     ]
    }
   ],
   "source": [
    "c =0.299792458\n",
    "n =1.3195\n",
    "v=c/n \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_d = []\n",
    "info_s = []\n",
    "data_d = []\n",
    "data_s = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    name_dd = \"/home/dup193/work/double_pulse/data/Tau05to15PeV_00{0}_data.npy\".format(i)\n",
    "    name_di = \"/home/dup193/work/double_pulse/data/Tau05to15PeV_00{0}_info.pkl\".format(i)\n",
    "    name_sd = \"/home/dup193/work/double_pulse/data/Electron05to15PeV_00{0}_data.npy\".format(i)\n",
    "    name_si = \"/home/dup193/work/double_pulse/data/Electron05to15PeV_00{0}_info.pkl\".format(i)\n",
    "    info_d_temp = pickle.load(open(name_di, \"rb\"))\n",
    "    info_s_temp = pickle.load(open(name_si, \"rb\"))\n",
    "    data_d_temp = np.load(name_dd ,allow_pickle=True,encoding='bytes')\n",
    "    data_s_temp = np.load(name_sd ,allow_pickle=True,encoding='bytes')\n",
    "    info_d = info_d + info_d_temp\n",
    "    info_s = info_s + info_s_temp\n",
    "    data_d.append(data_d_temp)    \n",
    "    data_s.append(data_s_temp)\n",
    "data_d = np.vstack(data_d)\n",
    "data_s = np.vstack(data_s)\n",
    "info_d = np.array(info_d)\n",
    "info_s = np.array(info_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10062 10062 10062 10062\n",
      "24019 24019 24019 24019\n"
     ]
    }
   ],
   "source": [
    "energy_l_d = []\n",
    "energy_nu_d = []\n",
    "charge_d = []\n",
    "charge_st_d = []\n",
    "for i in info_d:\n",
    "    energy_l_d.append(i['tau_energy'])\n",
    "    energy_nu_d.append(i['nu_energy'])\n",
    "    charge_d.append(i['qtotal'])\n",
    "    charge_st_d.append(i['strings']['charge'])\n",
    "\n",
    "energy_l_d = np.array(energy_l_d)\n",
    "energy_nu_d = np.array(energy_nu_d)\n",
    "charge_d = np.array(charge_d)\n",
    "charge_st_d = np.array(charge_st_d)\n",
    "\n",
    "energy_l_s = []\n",
    "energy_nu_s = []\n",
    "charge_s = []\n",
    "charge_st_s = []\n",
    "for i in info_s:\n",
    "    energy_l_s.append(i['tau_energy'])\n",
    "    energy_nu_s.append(i['nu_energy'])\n",
    "    charge_s.append(i['qtotal'])\n",
    "    charge_st_s.append(i['strings']['charge'])\n",
    "\n",
    "energy_l_s = np.array(energy_l_s)\n",
    "energy_nu_s = np.array(energy_nu_s)\n",
    "charge_s = np.array(charge_s)\n",
    "charge_st_s = np.array(charge_st_s)\n",
    " \n",
    "print(len(energy_l_d),len(energy_nu_d),len(charge_d),len(charge_st_d))\n",
    "print(len(energy_l_s),len(energy_nu_s),len(charge_s),len(charge_st_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot(arr1,arr2, bs =100, r = [0,100], lab = \"Charge\"):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    weights1 = np.ones_like(arr1)/float(len(arr1))\n",
    "    weights2 = np.ones_like(arr2)/float(len(arr2))\n",
    "    #ax.set_yscale(scale)\n",
    "    #ax.set_title(name, fontsize = 14)\n",
    "    ax.text(0.5,0.95,'#Events single '+str(len(arr1)), transform=ax.transAxes, color = \"black\",fontsize=8)\n",
    "    ax.text(0.5,0.90,'#Events double '+str(len(arr2)), transform=ax.transAxes, color = \"black\",fontsize=8)\n",
    "    ax.set_xlabel(lab, fontsize = 14)                                                              \n",
    "    ax.set_ylabel(\"FractionEvents\", fontsize = 14)                                                   \n",
    "    ax.hist(arr1, bins =bs, weights = weights1, range = r, histtype = 'step',edgecolor ='r', fill= False, label = 'Single')\n",
    "    ax.hist(arr2, bins =bs, weights = weights2, range = r, histtype = 'step',edgecolor ='b', fill= False, label = 'Double')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(energy_l_s/1000000,energy_l_d/1000000, bs =50, r = [0.4,1.6], lab = \"energy of lepton, GeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(energy_nu_s/1000000,energy_nu_d/1000000, bs =50, r = [0.4,3.6], lab = \"energy of neutrino, GeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(charge_s,charge_d,bs = 50, r= [0,200000], lab = \"total charge, Pe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(charge_st_s,charge_st_d,bs = 50, r= [0,200000], lab = \"string charge, Pe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10062,) (10062, 300, 60)\n",
      "(24019,) (24019, 300, 60)\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(info_d.shape, data_d.shape)\n",
    "print(info_s.shape, data_s.shape)\n",
    "print(type(data_d_temp[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.405907616717559e-09\n",
      "-5.962297593924894e-11\n",
      "6.883592770365534e-09\n",
      "-6.128851422642174e-11\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(data_d))\n",
    "print(np.amin(data_d))\n",
    "print(np.amax(data_s))\n",
    "print(np.amin(data_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10062, 2)\n",
      "(24019, 2)\n",
      "[0 1]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "label_d = [[0,1]]*len(data_d)\n",
    "label_s = [[1,0]]*len(data_s)\n",
    "label_d = np.array(label_d)\n",
    "label_s = np.array(label_s)\n",
    "print(label_d.shape)\n",
    "print(label_s.shape)\n",
    "print(label_d[0])\n",
    "print(label_s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0026542188 0.022200854\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate((data_d, data_s), axis = 0) \n",
    "label = np.concatenate((label_d, label_s), axis = 0) \n",
    "data, label = shuffle(data, label, random_state =12)\n",
    "\n",
    "train_data = data[:24000]\n",
    "train_label = label[:24000]\n",
    "train_data = train_data.reshape((len(train_data),300,60,1))\n",
    "train_data = train_data.astype('float32')/10**-8\n",
    "mean = np.mean(train_data)\n",
    "std = np.std(train_data)\n",
    "print(mean,std)\n",
    "train_data = train_data - mean\n",
    "train_data = train_data/std\n",
    "\n",
    "valid_data = data[24000:28000]\n",
    "valid_label = label[24000:28000]\n",
    "valid_data = valid_data.reshape((len(valid_data),300,60,1))\n",
    "valid_data = valid_data.astype('float32')/10**-8\n",
    "valid_data = valid_data - mean\n",
    "valid_data = valid_data/std\n",
    "\n",
    "test_data = data[28000:]\n",
    "test_label = label[28000:]\n",
    "test_data = test_data.reshape((len(test_data),300,60,1))\n",
    "test_data = test_data.astype('float32')/10**-8\n",
    "test_data = test_data - mean\n",
    "test_data = test_data/std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.886427\n",
      "-0.3950856\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(train_data))\n",
    "print(np.amin(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for im in data_d[:100]:\n",
    "#    print(im.shape)\n",
    "#    fig = plt.figure(figsize=(12, 12))\n",
    "#    ax = fig.add_subplot(111)\n",
    "#    ax.imshow(im, interpolation='nearest', aspect='auto', cmap= 'gray')\n",
    "    #ax.plot(im[14,:])\n",
    "    #print(im[:,46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 298, 58, 32)       320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 149, 29, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 147, 27, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 73, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 71, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 35, 5, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 33, 3, 128)        147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,322,306\n",
      "Trainable params: 1,322,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(300, 60, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "#              options = run_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "24000/24000 [==============================] - 8s 353us/step - loss: 0.6118 - acc: 0.7076 - val_loss: 0.6062 - val_acc: 0.7053\n",
      "Epoch 2/5\n",
      "24000/24000 [==============================] - 8s 351us/step - loss: 0.6015 - acc: 0.7076 - val_loss: 0.6040 - val_acc: 0.7053\n",
      "Epoch 3/5\n",
      "24000/24000 [==============================] - 9s 379us/step - loss: 0.5988 - acc: 0.7077 - val_loss: 0.6015 - val_acc: 0.7053\n",
      "Epoch 4/5\n",
      "24000/24000 [==============================] - 9s 367us/step - loss: 0.5981 - acc: 0.7078 - val_loss: 0.6015 - val_acc: 0.7050\n",
      "Epoch 5/5\n",
      "24000/24000 [==============================] - 9s 378us/step - loss: 0.5968 - acc: 0.7080 - val_loss: 0.5983 - val_acc: 0.7053\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,train_label, epochs=5, validation_data=(valid_data,valid_label), batch_size =128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "history_dict = history.history # Get the dictionary containing each metric and the loss for each epoch\n",
    "json.dump(history_dict, open('/data/dfc13/DoublePulse/Models/DP_model_10g.json', 'w')) # Save it under the form of a json file\n",
    "#\n",
    "model.save('/data/dfc13/DoublePulse/Models/DP_model_10g.h5') # Save the model\n",
    "#\n",
    "ResultsFile = open('/data/dfc13/DoublePulse/Models/DP_model_10g.txt','w') # File for saving the results of the fit.\n",
    "#\n",
    "# 8: multiple conv2d layers a la textbook (32 (3,3) then 64, 128, 128, dropout 0.3, dense 256(relu), 64(relu), 2(softmax)\n",
    "# 8b: same as 8 but with 50 trials\n",
    "# 8b[not c didn't update the line above]: same as 8b but with dropout 0.5.\n",
    "# 8c: changed SGC \"lr\" from 0.01->0.001.  This smoothed things out a lot.  Sort of.  But had 0% accuracy for nutaus.\n",
    "# 8d: changed SGC \"lr\" from 0.001->0.005.\n",
    "# 8e: decreased size of training set to 24k, increased size of validation set to 4k (24k:28k) and increased test set (28k:)\n",
    "# 8f: changed SGC \"lr\" from 0.005->0.01.  200 epochs. 97%/37%.\n",
    "# 8g: lr=0.02, 50 epochs: 98%/29%.\n",
    "\n",
    "# 9a: changed dense 256 -> 512.  50 epochs. 100%/22%\n",
    "# 9b: same as above, 200 epochs. 97%/39%.\n",
    "# 9c: same as above, lr = 0.01.  200 epochs. 97%/36%.\n",
    "# 9d: same as above, 500 epochs batch_size = 64. 96%/40%.\n",
    "# 10a: Testing selection of particular GPU.  20 epochs, batch_size = 128.\n",
    "# 10b: Testing parallelization.  50 epochs, batch_size = 128, one GPU. 98%/22%.\n",
    "# 10g: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "#plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "#plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "#plt.title('Training and validation accuracy')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.figure()\n",
    "\n",
    "#plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#plt.title('Training and validation loss')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6081/6081 [==============================] - 1s 241us/step\n",
      "[0.608180747205559, 0.693471468508469]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data,test_label)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_data)\n",
    "matrix = confusion_matrix(test_label.argmax(axis=1), test_pred.argmax(axis=1))\n",
    "report = classification_report(test_label.argmax(axis=1), test_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4216    0]\n",
      " [1864    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82      4216\n",
      "           1       1.00      0.00      0.00      1865\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      6081\n",
      "   macro avg       0.85      0.50      0.41      6081\n",
      "weighted avg       0.79      0.69      0.57      6081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"True Positive\",tp)\n",
    "#print(\"True Negative\",tn)\n",
    "#print(\"False Positive\",fp)\n",
    "#print(\"False Negative\",fn)\n",
    "print(matrix)\n",
    "print(report)\n",
    "\n",
    "ResultsFile.write(repr(matrix))\n",
    "ResultsFile.write('\\n')\n",
    "ResultsFile.write(report)\n",
    "ResultsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
