{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs Deep Learning on nutau and nue samples created by Dasha.  It runs on multiple GPUs at once.\n",
    "\n",
    "This code runs in a python 3.7 conda environment constructed as follows:\n",
    "- conda create --name tf-gpu tensorflow-gpu keras scikit-learn matplotlib ipykernel nb_conda_kernels [Pillow]\n",
    "- conda activate tf-gpu\n",
    "(Pillow is for image manipulation for making heat maps, but I haven't got it to work yet.  Can remove from environment.)\n",
    "\n",
    "To run with multiple GPUs on CyberLAMP you must specify the \"nodes\" and \"gpus\" qualifiers in the same chunk.  For example:\n",
    "- qsub -I -A cyberlamp -l qos=cl_higpu -l nodes=1:ppn=1:gpus=4:shared -l mem=24gb -l walltime=4:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set which GPU to use.  This probably needs to be done before any other CUDA vars get defined.\n",
    "# Use the command \"nvidia-smi\" to get association of a particular GPU with a particular number.\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2272015596816976\n"
     ]
    }
   ],
   "source": [
    "c =0.299792458\n",
    "n =1.3195\n",
    "v=c/n \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_d = []\n",
    "info_s = []\n",
    "data_d = []\n",
    "data_s = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    name_dd = \"/home/dup193/work/double_pulse/data/Tau05to15PeV_00{0}_data.npy\".format(i)\n",
    "    name_di = \"/home/dup193/work/double_pulse/data/Tau05to15PeV_00{0}_info.pkl\".format(i)\n",
    "    name_sd = \"/home/dup193/work/double_pulse/data/Electron05to15PeV_00{0}_data.npy\".format(i)\n",
    "    name_si = \"/home/dup193/work/double_pulse/data/Electron05to15PeV_00{0}_info.pkl\".format(i)\n",
    "    info_d_temp = pickle.load(open(name_di, \"rb\"))\n",
    "    info_s_temp = pickle.load(open(name_si, \"rb\"))\n",
    "    data_d_temp = np.load(name_dd ,allow_pickle=True,encoding='bytes')\n",
    "    data_s_temp = np.load(name_sd ,allow_pickle=True,encoding='bytes')\n",
    "    info_d = info_d + info_d_temp\n",
    "    info_s = info_s + info_s_temp\n",
    "    data_d.append(data_d_temp)    \n",
    "    data_s.append(data_s_temp)\n",
    "data_d = np.vstack(data_d)\n",
    "data_s = np.vstack(data_s)\n",
    "info_d = np.array(info_d)\n",
    "info_s = np.array(info_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10062 10062 10062 10062\n",
      "24019 24019 24019 24019\n"
     ]
    }
   ],
   "source": [
    "energy_l_d = []\n",
    "energy_nu_d = []\n",
    "charge_d = []\n",
    "charge_st_d = []\n",
    "for i in info_d:\n",
    "    energy_l_d.append(i['tau_energy'])\n",
    "    energy_nu_d.append(i['nu_energy'])\n",
    "    charge_d.append(i['qtotal'])\n",
    "    charge_st_d.append(i['strings']['charge'])\n",
    "\n",
    "energy_l_d = np.array(energy_l_d)\n",
    "energy_nu_d = np.array(energy_nu_d)\n",
    "charge_d = np.array(charge_d)\n",
    "charge_st_d = np.array(charge_st_d)\n",
    "\n",
    "energy_l_s = []\n",
    "energy_nu_s = []\n",
    "charge_s = []\n",
    "charge_st_s = []\n",
    "for i in info_s:\n",
    "    energy_l_s.append(i['tau_energy'])\n",
    "    energy_nu_s.append(i['nu_energy'])\n",
    "    charge_s.append(i['qtotal'])\n",
    "    charge_st_s.append(i['strings']['charge'])\n",
    "\n",
    "energy_l_s = np.array(energy_l_s)\n",
    "energy_nu_s = np.array(energy_nu_s)\n",
    "charge_s = np.array(charge_s)\n",
    "charge_st_s = np.array(charge_st_s)\n",
    " \n",
    "print(len(energy_l_d),len(energy_nu_d),len(charge_d),len(charge_st_d))\n",
    "print(len(energy_l_s),len(energy_nu_s),len(charge_s),len(charge_st_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot(arr1,arr2, bs =100, r = [0,100], lab = \"Charge\"):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    weights1 = np.ones_like(arr1)/float(len(arr1))\n",
    "    weights2 = np.ones_like(arr2)/float(len(arr2))\n",
    "    #ax.set_yscale(scale)\n",
    "    #ax.set_title(name, fontsize = 14)\n",
    "    ax.text(0.5,0.95,'#Events single '+str(len(arr1)), transform=ax.transAxes, color = \"black\",fontsize=8)\n",
    "    ax.text(0.5,0.90,'#Events double '+str(len(arr2)), transform=ax.transAxes, color = \"black\",fontsize=8)\n",
    "    ax.set_xlabel(lab, fontsize = 14)                                                              \n",
    "    ax.set_ylabel(\"FractionEvents\", fontsize = 14)                                                   \n",
    "    ax.hist(arr1, bins =bs, weights = weights1, range = r, histtype = 'step',edgecolor ='r', fill= False, label = 'Single')\n",
    "    ax.hist(arr2, bins =bs, weights = weights2, range = r, histtype = 'step',edgecolor ='b', fill= False, label = 'Double')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(energy_l_s/1000000,energy_l_d/1000000, bs =50, r = [0.4,1.6], lab = \"energy of lepton, GeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(energy_nu_s/1000000,energy_nu_d/1000000, bs =50, r = [0.4,3.6], lab = \"energy of neutrino, GeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(charge_s,charge_d,bs = 50, r= [0,200000], lab = \"total charge, Pe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot(charge_st_s,charge_st_d,bs = 50, r= [0,200000], lab = \"string charge, Pe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10062,) (10062, 300, 60)\n",
      "(24019,) (24019, 300, 60)\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(info_d.shape, data_d.shape)\n",
    "print(info_s.shape, data_s.shape)\n",
    "print(type(data_d_temp[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.405907616717559e-09\n",
      "-5.962297593924894e-11\n",
      "6.883592770365534e-09\n",
      "-6.128851422642174e-11\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(data_d))\n",
    "print(np.amin(data_d))\n",
    "print(np.amax(data_s))\n",
    "print(np.amin(data_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10062, 2)\n",
      "(24019, 2)\n",
      "[0 1]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "label_d = [[0,1]]*len(data_d)\n",
    "label_s = [[1,0]]*len(data_s)\n",
    "label_d = np.array(label_d)\n",
    "label_s = np.array(label_s)\n",
    "print(label_d.shape)\n",
    "print(label_s.shape)\n",
    "print(label_d[0])\n",
    "print(label_s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0026542188 0.022200854\n"
     ]
    }
   ],
   "source": [
    "data = np.concatenate((data_d, data_s), axis = 0) \n",
    "label = np.concatenate((label_d, label_s), axis = 0) \n",
    "data, label = shuffle(data, label, random_state =12)\n",
    "\n",
    "train_data = data[:24000]\n",
    "train_label = label[:24000]\n",
    "train_data = train_data.reshape((len(train_data),300,60,1))\n",
    "train_data = train_data.astype('float32')/10**-8\n",
    "mean = np.mean(train_data)\n",
    "std = np.std(train_data)\n",
    "print(mean,std)\n",
    "train_data = train_data - mean\n",
    "train_data = train_data/std\n",
    "\n",
    "valid_data = data[24000:28000]\n",
    "valid_label = label[24000:28000]\n",
    "valid_data = valid_data.reshape((len(valid_data),300,60,1))\n",
    "valid_data = valid_data.astype('float32')/10**-8\n",
    "valid_data = valid_data - mean\n",
    "valid_data = valid_data/std\n",
    "\n",
    "test_data = data[28000:]\n",
    "test_label = label[28000:]\n",
    "test_data = test_data.reshape((len(test_data),300,60,1))\n",
    "test_data = test_data.astype('float32')/10**-8\n",
    "test_data = test_data - mean\n",
    "test_data = test_data/std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.886427\n",
      "-0.3950856\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(train_data))\n",
    "print(np.amin(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 16:56:30.684806 139942403921664 deprecation_wrapper.py:119] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0815 16:56:30.687193 139942403921664 deprecation_wrapper.py:119] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0815 16:56:30.690948 139942403921664 deprecation_wrapper.py:119] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0815 16:56:30.710994 139942403921664 deprecation_wrapper.py:119] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0815 16:56:30.769237 139942403921664 deprecation_wrapper.py:119] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0815 16:56:30.776479 139942403921664 deprecation.py:506] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0815 16:56:30.822019 139942403921664 deprecation_wrapper.py:119] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0815 16:56:32.348149 139942403921664 deprecation_wrapper.py:119] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0815 16:56:32.438360 139942403921664 deprecation.py:323] From /home/dfc13/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 4000 samples\n",
      "Epoch 1/30\n",
      "24000/24000 [==============================] - 13s 550us/step - loss: 0.6238 - acc: 0.7015 - val_loss: 0.6118 - val_acc: 0.7052\n",
      "Epoch 2/30\n",
      "24000/24000 [==============================] - 5s 201us/step - loss: 0.6030 - acc: 0.7075 - val_loss: 0.6067 - val_acc: 0.7052\n",
      "Epoch 3/30\n",
      "24000/24000 [==============================] - 5s 215us/step - loss: 0.6003 - acc: 0.7076 - val_loss: 0.6055 - val_acc: 0.7052\n",
      "Epoch 4/30\n",
      "24000/24000 [==============================] - 6s 231us/step - loss: 0.6000 - acc: 0.7077 - val_loss: 0.6041 - val_acc: 0.7052\n",
      "Epoch 5/30\n",
      "24000/24000 [==============================] - 5s 222us/step - loss: 0.5982 - acc: 0.7078 - val_loss: 0.6037 - val_acc: 0.7052\n",
      "Epoch 6/30\n",
      "24000/24000 [==============================] - 5s 190us/step - loss: 0.5974 - acc: 0.7075 - val_loss: 0.6035 - val_acc: 0.7047\n",
      "Epoch 7/30\n",
      "24000/24000 [==============================] - 5s 228us/step - loss: 0.5962 - acc: 0.7078 - val_loss: 0.6040 - val_acc: 0.7050\n",
      "Epoch 8/30\n",
      "24000/24000 [==============================] - 5s 199us/step - loss: 0.5958 - acc: 0.7079 - val_loss: 0.6027 - val_acc: 0.7050\n",
      "Epoch 9/30\n",
      "24000/24000 [==============================] - 5s 210us/step - loss: 0.5949 - acc: 0.7088 - val_loss: 0.5996 - val_acc: 0.7053\n",
      "Epoch 10/30\n",
      "24000/24000 [==============================] - 5s 216us/step - loss: 0.5945 - acc: 0.7083 - val_loss: 0.5999 - val_acc: 0.7055\n",
      "Epoch 11/30\n",
      "24000/24000 [==============================] - 5s 200us/step - loss: 0.5930 - acc: 0.7085 - val_loss: 0.6024 - val_acc: 0.7055\n",
      "Epoch 12/30\n",
      "24000/24000 [==============================] - 5s 226us/step - loss: 0.5919 - acc: 0.7090 - val_loss: 0.6001 - val_acc: 0.7057\n",
      "Epoch 13/30\n",
      "24000/24000 [==============================] - 4s 184us/step - loss: 0.5900 - acc: 0.7094 - val_loss: 0.5964 - val_acc: 0.7067\n",
      "Epoch 14/30\n",
      "24000/24000 [==============================] - 5s 206us/step - loss: 0.5883 - acc: 0.7099 - val_loss: 0.5962 - val_acc: 0.7077\n",
      "Epoch 15/30\n",
      "24000/24000 [==============================] - 5s 204us/step - loss: 0.5877 - acc: 0.7104 - val_loss: 0.5963 - val_acc: 0.7080\n",
      "Epoch 16/30\n",
      "24000/24000 [==============================] - 4s 179us/step - loss: 0.5859 - acc: 0.7113 - val_loss: 0.5966 - val_acc: 0.7080\n",
      "Epoch 17/30\n",
      "24000/24000 [==============================] - 5s 218us/step - loss: 0.5842 - acc: 0.7125 - val_loss: 0.5993 - val_acc: 0.7082\n",
      "Epoch 18/30\n",
      "24000/24000 [==============================] - 5s 192us/step - loss: 0.5826 - acc: 0.7130 - val_loss: 0.5975 - val_acc: 0.7092\n",
      "Epoch 19/30\n",
      "24000/24000 [==============================] - 5s 194us/step - loss: 0.5806 - acc: 0.7139 - val_loss: 0.5929 - val_acc: 0.7113\n",
      "Epoch 20/30\n",
      "24000/24000 [==============================] - 5s 216us/step - loss: 0.5798 - acc: 0.7150 - val_loss: 0.5886 - val_acc: 0.7125\n",
      "Epoch 21/30\n",
      "24000/24000 [==============================] - 4s 179us/step - loss: 0.5778 - acc: 0.7155 - val_loss: 0.5915 - val_acc: 0.7130\n",
      "Epoch 22/30\n",
      "24000/24000 [==============================] - 5s 197us/step - loss: 0.5768 - acc: 0.7174 - val_loss: 0.5958 - val_acc: 0.7133\n",
      "Epoch 23/30\n",
      "24000/24000 [==============================] - 5s 215us/step - loss: 0.5744 - acc: 0.7192 - val_loss: 0.5910 - val_acc: 0.7145\n",
      "Epoch 24/30\n",
      "24000/24000 [==============================] - 4s 181us/step - loss: 0.5735 - acc: 0.7205 - val_loss: 0.5837 - val_acc: 0.7160\n",
      "Epoch 25/30\n",
      "24000/24000 [==============================] - 4s 181us/step - loss: 0.5732 - acc: 0.7198 - val_loss: 0.6034 - val_acc: 0.7130\n",
      "Epoch 26/30\n",
      "24000/24000 [==============================] - 5s 219us/step - loss: 0.5706 - acc: 0.7222 - val_loss: 0.5893 - val_acc: 0.7170\n",
      "Epoch 27/30\n",
      "24000/24000 [==============================] - 5s 191us/step - loss: 0.5693 - acc: 0.7242 - val_loss: 0.5737 - val_acc: 0.7230\n",
      "Epoch 28/30\n",
      "24000/24000 [==============================] - 5s 197us/step - loss: 0.5683 - acc: 0.7244 - val_loss: 0.5843 - val_acc: 0.7208\n",
      "Epoch 29/30\n",
      "24000/24000 [==============================] - 5s 214us/step - loss: 0.5665 - acc: 0.7251 - val_loss: 0.5779 - val_acc: 0.7245\n",
      "Epoch 30/30\n",
      "24000/24000 [==============================] - 4s 180us/step - loss: 0.5657 - acc: 0.7264 - val_loss: 0.5880 - val_acc: 0.7190\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import optimizers\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(300, 60, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "parallel_model = multi_gpu_model(model,gpus=3)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "parallel_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "history = parallel_model.fit(train_data,train_label, epochs=300, validation_data=(valid_data,valid_label), batch_size =192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "history_dict = history.history # Get the dictionary containing each metric and the loss for each epoch\n",
    "json.dump(history_dict, open('/data/dfc13/DoublePulse/Models/DP_model_10f-3.json', 'w')) # Save it under the form of a json file\n",
    "#\n",
    "model.save('/data/dfc13/DoublePulse/Models/DP_model_10f-3.h5') # Save the model\n",
    "#\n",
    "ResultsFile = open('/data/dfc13/DoublePulse/Models/DP_model_10f-3.txt','w') # File for saving the results of the fit.\n",
    "\n",
    "# 8: multiple conv2d layers a la textbook (32 (3,3) then 64, 128, 128, dropout 0.3, dense 256(relu), 64(relu), 2(softmax)\n",
    "# 8b: same as 8 but with 50 trials\n",
    "# 8b[not c didn't update the line above]: same as 8b but with dropout 0.5.\n",
    "# 8c: changed SGC \"lr\" from 0.01->0.001.  This smoothed things out a lot.  Sort of.  But had 0% accuracy for nutaus.\n",
    "# 8d: changed SGC \"lr\" from 0.001->0.005.\n",
    "# 8e: decreased size of training set to 24k, increased size of validation set to 4k (24k:28k) and increased test set (28k:)\n",
    "# 8f: changed SGC \"lr\" from 0.005->0.01.  200 epochs. 97%/37%.\n",
    "# 8g: lr=0.02, 50 epochs: 98%/29%.\n",
    "\n",
    "# 9a: changed dense 256 -> 512.  50 epochs. 100%/22%\n",
    "# 9b: same as above, 200 epochs. 97%/39%.\n",
    "# 9c: same as above, lr = 0.01.  200 epochs. 97%/36%.\n",
    "# 9d: same as above, 500 epochs batch_size = 64. 96%/40%.\n",
    "# 10a: Testing selection of particular GPU.  20 epochs, batch_size = 128.\n",
    "# 10b-3: Testing parallelization.  50 epochs, batch_size = 128, 3 GPUs (about 6s/epoch): 99%/19%.\n",
    "# 10c-3: Testing parallelization.  50 epochs, batch_size = 64*3=192, 3 GPUs (about 4.5s/epoch): 100%/11%.\n",
    "# 10d-3: Testing parallelization.  100 epochs, batch_size = 64*3=192, 3 GPUs (about 4s/epoch): 98%/29%.\n",
    "# 10e-3: Testing parallelization.  500 epochs, batch_size = 64*3=192, 3 GPUs (about 4s/epoch): 98%/29%; 98%/56%.\n",
    "# 10f-3: Parallel.  300 epochs, batch_size = 64*3=192, 3 GPUs. X%/Y%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "#plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "#plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "#plt.title('Training and validation accuracy')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.figure()\n",
    "\n",
    "#plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#plt.title('Training and validation loss')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6081/6081 [==============================] - 2s 263us/step\n",
      "[0.6084520763922358, 0.6938003617826015]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_model.evaluate(test_data,test_label)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = parallel_model.predict(test_data)\n",
    "matrix = confusion_matrix(test_label.argmax(axis=1), test_pred.argmax(axis=1))\n",
    "report = classification_report(test_label.argmax(axis=1), test_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4215    1]\n",
      " [1861    4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82      4216\n",
      "           1       0.80      0.00      0.00      1865\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      6081\n",
      "   macro avg       0.75      0.50      0.41      6081\n",
      "weighted avg       0.73      0.69      0.57      6081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"True Positive\",tp)\n",
    "#print(\"True Negative\",tn)\n",
    "#print(\"False Positive\",fp)\n",
    "#print(\"False Negative\",fn)\n",
    "\n",
    "ResultsFile.write(repr(matrix))\n",
    "ResultsFile.write('\\n')\n",
    "ResultsFile.write(report)\n",
    "ResultsFile.close()\n",
    "\n",
    "print(matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
